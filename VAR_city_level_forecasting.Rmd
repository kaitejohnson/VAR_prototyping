---
title: "state space VAR city level forecasting"
output: html_document
date: "2025-02-10"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(mvgam)           # Fit, interrogate and forecast DGAMs
library(tidyverse)       # Tidy and flexible data manipulation
library(ggplot2)         # Flexible plotting
library(tidybayes)       # Graceful plotting of Bayesian posterior estimates
library(farver)          # Colour space manipulations
library(lubridate)       # Date formatting
library(forecasttools)   # Helper functions for forecast formatting

theme_set(theme_classic(base_size = 15,
                        base_family = 'serif'))
myhist = function(...){
  geom_histogram(col = 'white',
                 fill = '#B97C7C', ...)
}
hist_theme = function(){
  theme(axis.line.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank())
}
```
## Introduction

This is based on the `mvgam` tutorial on [state-space vector autoregressions](https://ecogambler.netlify.app/blog/vector-autoregressions/),
as well as the code developed to fit the portal data in the both the 
[portal_VAR](https://github.com/nicholasjclark/portal_VAR) repository and 
the [distributed lags tutorial](https://ecogambler.netlify.app/blog/distributed-lags-mgcv/).

The first tutorial uses data from multiple locations fit to a single species of
birds in Canada with no additional covariantes, the second two projects are fit 
to data on differet species of rats with additional covariates of minimum 
temperature and vegetation index. 

We're going to treat year, week, and day of the week as our covariates, and
the different boroughs as our different series, analagous to to the species or
locations in the above examples. 

We're going to take a first pass at fitting an time series (AR1, ARIMA, VARIMA) 
to the city level 
forecasting data from NYC available as part of the [Flu Metrocast Hub](https://github.com/reichlab/flu-metrocast).
We'll start by grabbing a vintage of the data from January 3rd, 2025. 

## Load the data

This is from a dataset containing counts of new ED visits due to ILI from each of
the 5 boroughs, plus citywide admissions and unknown admissions, from March of 2017 
through the day before the forecast date (January 2nd, 2025). 

We'll start by truncating the data to after October 2023 for consistency with 
the NSSP data, to exclude COVID pandemic years, and to speed up model run time. 
We will likely want to add the data back in later 
```{r}
forecast_date <- "2025-01-03"
raw_data <- read.csv("https://raw.githubusercontent.com/reichlab/flu-metrocast/refs/heads/main/raw-data/NYC_ED_daily_asof_01-03-2025.csv")
head(raw_data)

data_formatted <- raw_data |>
  mutate(
    date = as.Date(Date, format = "%m/%d/%Y") + years(2000),
    count = as.integer(X),
    series = as.factor(Dim1Value),
    # Eventually we will want to scale these (compute z scores) but leave for now
    year = year(date), 
    week = week(date),
    day_of_week = wday(date)
  ) |>
  filter(Dim2Value == "All age groups") |>
        ## date >= "2023-10-01",) |>
  rename(location = Dim1Value) |>
  mutate(year = year - min(year) + 1,
          time = as.integer(date - min(date) + 1)) |> # rescale year
  select(time, date, count, series, location, year, week, day_of_week) |>
  # Exclude the covid years
  filter(!date %in% seq(from = ymd("2020-02-01"), 
                        to = ymd("2022-03-01"), 
                        by = "day"))

   

ggplot(data_formatted) +
  geom_line(aes(x = time, y = count)) +
  facet_wrap(~series, scales = "free_y") +
  theme_bw()

```
Inspect some associations between log(counts + 1) and year, week, and day of week
for each location to get a sense of how their relative values vary
over seasons, years, and days
```{r}

# Plot some smooths to see how these variables change with time 
loc_to_plot <- 'Citywide'
data_formatted |>
  filter(location == loc_to_plot) |>
  ggplot(aes(x = week, y = log(count + 1))) +
  geom_point() +
  geom_smooth(method = "gam", formula = y ~ s(x, k = 12),
              col = 'darkred', fill = "#A25050") +
  labs(title =  {{loc_to_plot}},
       y = "log(count)", 
       x = 'week') +
  data_formatted |>
  filter(location == loc_to_plot) |>
  ggplot(aes(x = year, y = log(count + 1))) +
  geom_point() +
  geom_smooth(method = "gam", formula = y ~ s(x, k = 3),
              col = 'darkred', fill = "#A25050") +
  labs(title =  {{loc_to_plot}},
       y = "log(count)", 
       x = 'year') +
  data_formatted |>
  filter(location == loc_to_plot) |>
  ggplot(aes(x = day_of_week, y = log(count + 1))) +
  geom_point() +
  geom_smooth(method = "gam", formula = y ~ s(x, k = 3),
              col = 'darkred', fill = "#A25050") +
  labs(title = {{loc_to_plot}},
       y = "log(count)", 
       x = 'day of week') 
  
```

```{r}
plot_mvgam_series(data = data_formatted,
                  y = "count",
                  series = "all")

plot_mvgam_series(data = data_formatted,
                  y = "count",
                  series = 1)
```
## Data wrangling

We can't have any missing values in the predictors, so we will have to make 
sure that time is complete for all locations 
```{r}
# Expand to have rows for all date-location combinations, 
# even if they are missing a count value (since that is ok). 
model_data <- data_formatted |>
  group_by(series, location) |>
  complete(date = seq(min(date), max(date), by = "day")) |>
  ungroup() |>
  # Remake the predictors to fill in the missing ones 
  mutate(
    year = year(date), 
    week = week(date),
    day_of_week = wday(date)
  ) |>
  mutate(year = year - min(year) + 1,
          time = as.integer(date - min(date) + 1)
         )|>
 select(time, date, count, series, location, year, week, day_of_week)


ggplot(model_data) +
  geom_line(aes(x = time, y = count)) +
  facet_wrap(~series, scales = "free_y") +
  theme_bw()

# Make a plot of the time series overlaid to see if can visually see leading 
# vs lagging trends 
ggplot(model_data |> group_by(location) |>
         mutate(rel_count = count/max(count, na.rm = TRUE)))+
  geom_line(aes(x = time, 
                y = rel_count, 
                color = location), alpha = 0.3) +
  theme_bw()




```

## Fit a VAR(1) model
We will use the `trend_formula()` argument to specify we want a state-space 
model, and we can use the `trend_model()` to specify that we want the latent 
process to evolve as Vector Autoregression of order 1. This takes advantage of
developments in the field that enforce stationarity through a principled prior
on the autoregressive coefficients. Because magnitude of average coutns varies 
across regions, we will need to include region level intercepts in the process 
model. And since the response variable is positive integered values, we can
choose to use a Poisson distribution for the observation model.

First, create the forecast data by extending the dates for all locations
28 days into the future, which should cover completely the 4 week horizons 
we want to forecast. 
```{r}
forecast_days <- 30
forecast_data <- model_data |>
  group_by(series, location) |>
  tidyr::complete(date = seq(from = max(date) + days(1), 
                             to = max(date) + days(forecast_days),
                             by = "days")) |>
  ungroup() |>
  mutate(
    year = year(date), 
    week = week(date),
    day_of_week = wday(date)
  ) |>
  mutate(year = year - min(year) + 1,
          time = as.integer(date - min(date) + 1)
         ) |>
  filter(date > max(model_data$date))
```

## Model 1: Dynamic hierarchical GAM
Define a state-space dynamical hierarchical GAM. Start using AR1 trend model,
later can look into a vector autoregression which will jointly estimate the
AR coefficients across the locations... 
$$
y_{l,t} \sim Poisson(exp(x_{l,t})) \\
x_{l,t} \sim MVN(\mu_{l,t} + \delta_{l} x_{l,t-1},  \Sigma_{process})\\
\mu_{l,t} = \beta_l + f_{global,t}(week) + f_{l,t}(week) + f_{global,t}(wday) \\
\beta_l \sim Normal(\beta_{global}, \sigma_{count}) \\
\beta_{global} \sim Normal(log(avgcount), 1) \\
\sigma_{count} \sim exp(0.33) \\
\delta_l \sim Normal(0.5, 0.25) \\
\Sigma_{process} = \sigma * C * \sigma \\
\sigma \sim exp(1) \\
C \sim LKJcorr(2)
$$
Start by getting the default parameters and modifying
```{r}
def_priors <- get_mvgam_priors(formula = count ~ -1,
                               trend_formula = ~ 
                                 s(trend, bs = "re") + 
                                 s(week, k = 12) +
                                 s(week, trend, bs = "sz", k = 12) - 1 +
                                 s(day_of_week, k = 3),
                               trend_model = 'AR1',
                               data = model_data,
                               family = poisson())
```
Inform prior on the global mean of the intercept using the data 
```{r}

print(log(mean(model_data$count, na.rm = TRUE)))
```



```{r}

ar_mod <- mvgam(
  # Observation formula, empty to only consider the Gamma observation process
  formula = count ~ -1,
  
  # Process model formula that includes regional intercepts
  trend_formula = ~ 
    # Hierarchical intercepts capture variation in average count
    s(trend, bs = "re") + 
    # Hierarchical effects of year(shared smooth)
    # s(year, k = 3) +
    # # Borough level deviations
    # s(year, trend, bs = "sz", k = 3) -1 + 
    
    # Hierarchical effects of week(shared smooth)
    s(week, k = 12) +
    # Borough level deviations 
    s(week, trend, bs = "sz", k = 12) - 1 +
    
    # Shared smooth of day of week 
    s(day_of_week, k = 3),
  trend_model = 'AR1',
  #Adjust the priors 
  priors = c(
    prior(normal(4.4, 1), class = mu_raw_trend),
    prior(exponential(0.33), class = sigma_raw_trend),
  
    prior(exponential(1), class = sigma),
    prior(normal(0.5, 0.25), class = ar1, lb = -1, ub = 1)),
  data = model_data,
  newdata = forecast_data,
  backend = 'cmdstanr',
  family = poisson()
)
```
Look at some things
```{r}

summary(ar_mod)

plot_predictions(ar_mod,
                 condition = c('week', 'series'),
                 points = 0.5, conf_level = 0.5) +
  labs(y = "Counts", x = "week")

plot_predictions(ar_mod,
                 condition = c('week', 'day_of_week', 'series'),
                 points = 0.5, conf_level = 0.5) +
  labs(y = "Counts", x = "week")

conditional_effects(ar_mod)
mcmc_plot(ar_mod,
          variable = 'sigma',
          regex = TRUE,
          type = 'trace')

mcmc_plot(ar_mod, variable = 'ar1', regex = TRUE, type = 'areas')
plot_slopes(ar_mod,
            variable = 'week',
            condition = c('series', 'series'),
            type = 'link') +
  theme(legend.position = 'none') +
  labs(y = 'Log(counts)', x = 'Location')

# Hierarchical trend effects
plot(ar_mod, type = 'smooths', trend_effects = TRUE)
# Hierarchical intercepts ?? Actually not sure 
plot(ar_mod, type = 're', trend_effects = TRUE)

```
# Plot the forecasts and the residuals 
```{r}
# Fit and forecasts
plot(ar_mod, type = 'forecast', series = 1)
plot(ar_mod, type = 'forecast', series = 2)
plot(ar_mod, type = 'forecast', series = 3)
plot(ar_mod, type = 'forecast', series = 4)
plot(ar_mod, type = 'forecast', series = 5)
plot(ar_mod, type = 'forecast', series = 6)
plot(ar_mod, type = 'forecast', series = 7)

plot(ar_mod, type = 'residuals', series = 1)
plot(ar_mod, type = 'residuals', series = 2)
plot(ar_mod, type = 'residuals', series = 3)
plot(ar_mod, type = 'residuals', series = 4)
plot(ar_mod, type = 'residuals', series = 5)
plot(ar_mod, type = 'residuals', series = 6)
plot(ar_mod, type = 'residuals', series = 7)
```
Let's pretend we wanted to submit the forecasts. We'd need to extract the
posterior draws from only the forecast horizon/dates we want to forecast, 
aggregate into weekly sums by draw, and then summarize into quantiles. 
First, let's grab the draws and make some plots to make sure things look ok. 
```{r}
forecast_obj <- forecast(ar_mod, newdata = forecast_data, type = "response")

# Write a function to take in the model data and the forecast object output
# from mvgam and return a dataframe with the calibration and forecast data 
# with the dates and data bound to it. 
make_long_pred_df <- function(forecast_obj, 
                              model_data){
  
for (i in 1:length(unique(model_data$location))){
  matrix_preds <- forecast_obj$forecasts[[i]]
  matrix_hindcasts <- forecast_obj$hindcasts[[i]]
  df_i <- as.data.frame(matrix_preds) |>
    mutate(draw = seq(from = 1, to = nrow(matrix_preds))
    ) 
  colnames(df_i) <- c(as.character(
                        seq(from = 1, to = ncol(matrix_preds),
                            by = 1)), "draw")
  df_i <- df_i |> 
    pivot_longer(!draw,
                 names_to = "t",
                 values_to = "count") |>
    mutate(location = unique(model_data$location)[i],
           t = as.integer(t) + ncol(matrix_hindcasts),
           period = "forecast")
  
  dfhind_i <- as.data.frame(matrix_hindcasts) |>
    mutate(draw = seq(from = 1, 
                      to = nrow(matrix_hindcasts))
    ) 
  colnames(dfhind_i) <- c(as.character(
                        seq(from = 1, to = ncol(matrix_hindcasts),
                            by = 1)), "draw")
  dfhind_i <- dfhind_i |> 
    pivot_longer(!draw,
                 names_to = "t",
                 values_to = "count") |>
    mutate(location = unique(model_data$location)[i],
           t = as.integer(t),
           period = "hindcast")
  if(i ==1){
    df <- df_i
    df_hind <- dfhind_i
  }else{
    df <- bind_rows(df, df_i)
    df_hind <- bind_rows(df_hind, dfhind_i)
  }
}

dfall <- bind_rows(df, df_hind)
dfall <- dfall |>
  left_join(data.frame(
    t = 1:max(dfall$t),
    date = seq(from = min(model_data$date),
               to = min(model_data$date) + max(dfall$t) - 1,
               by = "day")
  ), by = "t") |>
  left_join(model_data |>
              rename(obs_count = count) |>
              select(date, obs_count, location),
            by = c("date", "location")
  )

return(dfall)
} 

dfall <- make_long_pred_df(forecast_obj, model_data)

# Quick plot
sampled_draws <- sample(1:max(dfall$draw), 100)
ggplot(dfall |> filter(draw %in% c(sampled_draws),
                       date >= "2024-10-01"))+
  geom_line(aes(x = date, y = count, 
                group = draw, 
                color = period),
            alpha = 0.2) + 
  coord_cartesian(xlim= ) +
  facet_wrap(~location, scales = "free_y")


```
Next, we have to summarize our predictions from daily to weekly, at the level
of epiweeks. Luckily, the there's a function for `epiweek` in the
`lubridate` package.
```{r}

# Write a function to convert data + model trajectories from daily to weekly
# for now assume `count` is column name of model values, `obs_count` is 
# column name of the observed data. Assumes a forecast date that creates a 
# reference date for the next Saturday. 

daily_to_epiweekly_data <- function (dfall, forecast_date) {
  df_forecasts <- dfall |>
  mutate(epiweek = epiweek(date),
         year = year(date),
         reference_date = ymd(forecast_date) + 
           (7- wday(ymd(forecast_date), week_start = 7)),
         target_end_date = ymd(date) + (7 - wday(date, week_start = 7))) |>
  arrange(date) 

df_weekly <- df_forecasts |>
  group_by(
    reference_date,
    target_end_date,
    location,
    draw
  ) |>
  summarize(
    n_days_data =  n(),
    count_7d = sum(count),
    obs_weekly_sum = sum(obs_count)
  ) |>
  ungroup() |>
  dplyr::mutate(
    horizon = floor(as.integer(target_end_date - reference_date))/7
  )

return(df_weekly)
}

df_weekly <- daily_to_epiweekly_data(dfall, forecast_date)

if(!all(df_weekly$n_days_data[df_weekly$horizon >=0] == 7)){
  cli::cli_abort(
    message = "Not all weeks contain 7 days of data"
  )
}

format_NYC_forecasts <- function(df_weekly){
  df_weekly_quantiled <- df_weekly |> 
  trajectories_to_quantiles( timepoint_cols = c("target_end_date"),
                             value_col = "count_7d",
                             id_cols = c("location", "reference_date",
                                         "horizon", "obs_weekly_sum")) |>
  mutate(output_type = "quantile",
         target = "ILI ED visits",
         location = ifelse(location == "Citywide", "NYC", location)) |>
  rename(output_type_id = quantile_level,
         value = quantile_value) |>
  select(reference_date, location, horizon, obs_weekly_sum, 
         target, target_end_date,
         output_type, output_type_id, value)
  
  return(df_weekly_quantiled)
}

df_weekly_quantiled <- format_NYC_forecasts(df_weekly)
```
Make a quick plot of the forecasts vs the data up until the forecast date
```{r}
df_quantiles_wide <- df_weekly_quantiled |>
  filter(target_end_date >= reference_date, 
    output_type_id %in% c(0.5, 0.025, 0.975, 0.25, 0.75)) |>
  tidyr::pivot_wider(id_cols = c("location", "target_end_date"),
                     names_from = "output_type_id") 

 
  
ggplot() +
  geom_line(data = df_weekly_quantiled |>
              filter(target_end_date >= reference_date - weeks(10)),
            aes(x= target_end_date, y = obs_weekly_sum)) +
    geom_point(data = df_weekly_quantiled |>
                 filter(target_end_date >= reference_date - weeks(10)),
            aes(x = target_end_date, y = obs_weekly_sum)) +
  facet_wrap(~location, scales = "free_y") +
  geom_line(data = df_quantiles_wide,
            aes(x = target_end_date, y = `0.5`)) +
  geom_ribbon(data = df_quantiles_wide,
              aes(x = target_end_date,
                  ymin = `0.25`,
                  ymax = `0.75`),
              alpha = 0.2) + 
  geom_ribbon(data = df_quantiles_wide,
              aes(x = target_end_date,
                  ymin = `0.025`,
                  ymax = `0.975`),
              alpha = 0.2) +
  xlab("") +
  ylab("ILI ED visits") +
  ggtitle("Dynamic GAM fit to data from 2016")
 

```


  


  
  
```


## Model 2: Vector autorgression 
Define a state-space dynamical hierarchical GAM with a vector autoregression 
to account for dependencies between the locations

Note this is not working as expected, likely something wrong with the VAR since
that is the only major change from the previous model. 
$$
y_{l,t} \sim Poisson(exp(x_{l,t})) \\
x_{l,t} \sim MVN(\mu_{l,t} + A * x_{l,t-1},  \Sigma_{process})\\
\mu_{l,t} = \beta_l +  f_{global}(week) \\
\beta_l \sim Normal(\beta_{global}, \sigma_{count}) \\
\beta_{global} \sim Normal(log(avgcount), 1) \\
\Sigma_{process} = \sigma * C * \sigma \\
A \exists P(R)\\
P \sim Normal(0, 0.67) \\
\sigma \sim exp(1) \\
C \sim LKJcorr(2)


$$
```{r}
def_priors_var <- get_mvgam_priors(formula = count ~ -1,
                                   trend_formula = ~ location,
                                   s(trend, week, k = 12) + 
                                    trend_model = VAR(cor = TRUE),
                                    data = model_data,
                                    family = poisson())
```
Note this model is a lot slower because of all the off-diagonal 
components of A that we are now estimating!
```{r}
var_mod <- mvgam(
  # Observation formula, empty to only consider the Gamma observation process
  formula = count ~ -1,

  # Process model formula that includes regional intercepts and global weekly
  # trend 
  trend_formula = ~ 
  s(trend, bs = "re") + 
  s(week, k = 12), 
  trend_model = VAR(cor = TRUE),
  #Adjust the priors 
  priors = c(prior(std_normal(), class = Intercept_trend),
            prior(exponential(2.5), class = sigma),
            prior(normal(4.7, 1), class = mu_raw_trend)),
  data = model_data,
  newdata = forecast_data,
  backend = 'cmdstanr',
  family = poisson(),
  burnin = 250,
  samples = 500
)
```
Make the same plots as previously
```{r}
summary(var_mod)
plot(var_mod, type = 'smooths', trend_effects = TRUE)
# Hierarchical intercepts ?? Actually not sure 
plot(var_mod, type = 're', trend_effects = TRUE)

plot_predictions(var_mod,
                 condition = c('week', 'series'),
                 points = 0.5, conf_level = 0.5) +
  labs(y = "Counts", x = "week")


conditional_effects(var_mod)
mcmc_plot(var_mod,
          variable = 'sigma',
          regex = TRUE,
          type = 'trace')

plot_slopes(var_mod,
            variable = 'week',
            condition = c('series', 'series'),
            type = 'link') +
  theme(legend.position = 'none') +
  labs(y = 'Log(counts)', x = 'Location')

# Hierarchical trend effects
plot(var_mod, type = 'smooths', trend_effects = TRUE)
# Hierarchical intercepts ?? Actually not sure 
plot(var_mod, type = 're', trend_effects = TRUE)

```
  
 # Plot the forecasts and the residuals 
```{r}
# Fit and forecasts
plot(var_mod, type = 'forecast', series = 1)
plot(var_mod, type = 'forecast', series = 2)
plot(var_mod, type = 'forecast', series = 3)
plot(var_mod, type = 'forecast', series = 4)
plot(var_mod, type = 'forecast', series = 5)
plot(var_mod, type = 'forecast', series = 6)
plot(var_mod, type = 'forecast', series = 7)

plot(var_mod, type = 'residuals', series = 1)
plot(var_mod, type = 'residuals', series = 2)
plot(var_mod, type = 'residuals', series = 3)
plot(var_mod, type = 'residuals', series = 4)
plot(var_mod, type = 'residuals', series = 5)
plot(var_mod, type = 'residuals', series = 6)
plot(var_mod, type = 'residuals', series = 7)
```

Compare models
```{r}
loo_compare(ar_mod, var_mod)
```
 
Other options to add:
- could try the first version as an ARIMA, second as a VARIMA? 
using `trend_model = AR(p = 1, d = 1, q = 1) /VAR (p = 1, d = 1, q = 1)`
- could fit to the full time series (expect would take a lot longer, but 
need to test this as I don't think it would increase the number of parameters)
- Could try getting rid of the day of week effect and just aggregating the data
to weekly and fitting that, excluding the last two days since they are an 
incomplete epiweek (but feels like info lost)
- different observation models e.g. negative binomial 

